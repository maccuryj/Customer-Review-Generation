@online{amazondataset,
	author    = "Amazon Web Services",
	title     = "Amazon Customer Review Dataset",
	url       = "https://s3.amazonaws.com/amazon-reviews-pds/readme.html",
	keywords  = "amazon,dataset"
}
@online{karpathy,
	author    = "Andrej Karpathy",
	title     = "The Unreasonable Effectiveness of Recurrent Neural Networks",
	url       = "http://karpathy.github.io/2015/05/21/rnn-effectiveness/",
	keywords  = "rnn, text generation"
}
@InProceedings{distmetricsbehavior,
	author="Aggarwal, Charu C.
	and Hinneburg, Alexander
	and Keim, Daniel A.",
	editor="Van den Bussche, Jan
	and Vianu, Victor",
	title="On the Surprising Behavior of Distance Metrics in High Dimensional Space",
	booktitle="Database Theory --- ICDT 2001",
	year="2001",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="420--434",
	abstract="In recent years, the effect of the curse of high dimensionality has been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from a effciency and/or effectiveness perspective. Recent research results show that in high dimensional space, the concept of proximity, distance or nearest neighbor may not even be qualitatively meaningful. In this paper, we view the dimensionality curse from the point of view of the distance metrics which are used to measure the similarity between objects. We specifically examine the behavior of the commonly used Lknorm and show that the problem of meaningfulness in high dimensionality is sensitive to the value of k. For example, this means that the Manhattan distance metric L(1norm) is consistently more preferable than the Euclidean distance metric L(2norm) for high dimensional data mining applications. Using the intuition derived from our analysis, we introduce and examine a natural extension of the Lknorm to fractional distance metrics. We show that the fractional distance metric provides more meaningful results both from the theoretical and empirical perspective. The results show that fractional distance metrics can significantly improve the effectiveness of standard clustering algorithms such as the k-means algorithm.",
	isbn="978-3-540-44503-6"
}
@article{breiman2001,
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to },
	added-at = {2015-04-15T08:57:31.000+0200},
	author = {Breiman, Leo},
	biburl = {https://www.bibsonomy.org/bibtex/2b8187107bf870043f2f93669958858f1/kdepublication},
	description = {Random Forests - Springer},
	doi = {10.1023/A:1010933404324},
	interhash = {4450d2e56555e7cb8f3817578e1dd4da},
	intrahash = {b8187107bf870043f2f93669958858f1},
	issn = {0885-6125},
	journal = {Machine Learning},
	keywords = {classification classifier dblp decision ensemble final forest forests imported kde learning machine ml mykopie origin random text-detection the_youtube_social_network thema:exploiting_place_features_in_link_prediction_on_location-based_social_networks trees uw_ss14_web2.0},
	language = {English},
	number = 1,
	pages = {5-32},
	publisher = {Kluwer Academic Publishers},
	timestamp = {2015-04-24T14:37:24.000+0200},
	title = {Random Forests},
	url = {http://dx.doi.org/10.1023/A%3A1010933404324},
	volume = 45,
	year = 2001
}